Part 1

1.1

1. Why does it take this many threads or iterations?

It takes about 500 iterations for there to be a consistent
ERROR for any amount of threads greater than 1. At this many
iterations, it is likely that threads will overlap each other
when they are doings the adds, and some of the adds will be
ignored, thus making the count not equal 0.

The number of threads needed to to make the program consistently
fail depends greatly on the iterations. If your iterations have
a value less than 10, you can run almost as many threads as you
would like. The less threads there are, the less likely there
will be an overlap between them, so it will fail less. But they all
consistently fail at the 500 iteration mark.

2. Why does a significantly smaller number of iterations so seldom fail?

If the number of iterations is very low, then each thread
will run for a significantly short amount of time, so the
probability of each thread overlapping each other is minimized.


For Graph: (1 thread)
iterations	per operation
10		13854	ns
25		6547	ns
50		4186	ns
100		1460	ns
250		688	ns
500		287	ns
1000		139	ns
2500		81	ns
5000		33	ns
10000		22	ns
25000		12	ns
50000		8	ns
100000		7	ns
250000		6	ns
500000		6	ns
1000000		5	ns
10000000	5	ns
100000000	5	ns


1.2

1. Why does the average cost per operation drop with increasing iterations?

There is less overhead relative to the amount of work that needs
to be done.

2. How do we know what the “correct” cost is?

The correct cost is 5 ns. We know this because we ran with
1000000+ iterations, we got this value everytime. To find the
correct cost, we want a really high iteration value so that
the overhead from creating the threads and such is negligible
compared to the runtime of the additions and subtractions.

3. Why are the --yield runs so much slower? Where is the extra time going?

These runs are so much slower because each thread stops running
after each add it does, and lets another thread take over.
This means we are switching between threads a lot, and are
adding a lot of overhead. The time taken to switch between threads
after every add operation is where the extra time is coming from.

4. Can we get valid timings if we are using --yield? How, or why not?

No, you cannot get valid timings if we use --yield. This adds
so much overhead because it causes a thread switch after every add.
All this overhead will add a lot of extra time and make the
timings invalid.


We used 100000 iterations, because it will lower overhead without taking
excessive time to run.

Threads		unprotected	pthread_mutex	spinlock	atomic
1		7	ns	29	ns	15	ns	20	ns
2		8	ns	48	ns	68	ns	45	ns
3		13	ns	133	ns	124	ns	75	ns
4		12	ns	176	ns	152	ns	87	ns
5		12	ns	212	ns	243	ns	93	ns
6		13	ns	184	ns	251	ns	104	ns
8		12	ns	145	ns	331	ns	107	ns
10		13	ns	139	ns	412	ns	106	ns
16		13	ns	125	ns	684	ns	115	ns
32		16	ns	122	ns	1337	ns	150	ns

1.3

1. Why do all of the options perform similarly for low numbers of threads?

Because there are so few threads, there is less chance for a race
condition, so the overhead from the locking is minimal and affects
the time minimally as well. Thus all of them run similarly to the
original run times.

2. Why do the three protected operations slow down as the number of threads rises?

As the number of threads rises, there are more race conditions,
and threads have to wait for one another more often, so there is
more wait time, and thus the overall speed of the program slows
down.

3. Why are spin-locks so expensive for large numbers of threads?

Spinlocks are busy waiters, so they do not suspend when the lock
is already taken, but instead continually check if the lock has been
unlocked. This wastes CPU time and increases the time of our proram
greatly. As more threads are added, this wait time becomes longer and
thus more expensive.

PART 2

For Graph: (1 thread) (linked list)
iterations	per operation
10		2396	ns
25		401	ns
50		112	ns
100		38	ns
250		12	ns
500		6	ns
1000		5	ns
2500		5	ns
5000		6	ns
10000		8	ns
25000		10	ns
50000		11	ns
100000		11	ns
250000		13	ns

2.1

Explain the variation in time per operation vs the number of
iterations? How would you propose to correct for this effect?

For low iterations, the overhead from creating the threads is affecting
the time per operation. For higher iterations, the linked list traversal
adds overhead as well. Thus we get the most accurate time per
operation around 2000 iterations with 1 thread. To lower the overhead
from creating the threads, we could have each thread measure its own
time rather than the main process. This way we do not time the creation
of the threads, but just the operations done by each thread. The overhead
from the linked list cannot be fixed, unless we change our data structure.

We found the "correct" number of iterations needed to be ~2000

Threads		per operation
		unprotected	pthread_mutex	spinlock
1		5	ns	7	ns	7	ns
2		NULL	ns	16	ns	9	ns
3		NULL	ns	23	ns	21	ns
4		NULL	ns	31	ns	28	ns
5		NULL	ns	34	ns	32	ns
6		NULL	ns	43	ns	48	ns
8		NULL	ns	60	ns	59	ns
10		NULL	ns	65	ns	64	ns
16		NULL	ns	117	ns	132	ns
32		NULL	ns	214	ns	324	ns

2.2

Compare the variation in time per protected operation vs
the number of threads in Part 2 and in Part 1. Explain the difference.

Part 1 (pthread_mutex): Peaked at 5 threads (212 ns) and got faster.
Part 2 (pthread_mutex): gradually increased with thread number.

We believe that the difference of pattern comes from how the
kernel's scheduling algorithm handles x amount of threads to do
operations that take y amount of time. For instance, the algorithm
can be very effective if it runs processes in an order where there
is very little wait time, and inefficient if it runs processes so
that they are continually waiting on each other. A lot of factors
can affect whether this alogorithm is efficient or not, and one of
those is how long a process will hold the lock. In part 1, the lock
is held very briefly while in part 2 it is held for a long time.
This aspect of each part can explain why there is a different pattern
as they each got different values for the overhead from the scheduling
algorithm.

Part 1 (spinlock): greatly increased with thread number
Part 2 (spinlock): gradual increased with thread number

Unlike for pthread_mutex, spinlocks always cause busy waiting, and 
cause a lot of overhead, but this overhead is consistent, so that
is why the patterns for part 1 and part 2 are very similar.
In part 1, the spinlock is called for every add operation,
where as in part 2, the spinlock is called for every transversal
of the list. Each transversal is many operations, so the overhead
from the spinlock is split amount them, so each operation becomes
cheaper than they were in part 1. In both cases, the spinlock is
the majority of the time for the operation, but it holds more weight
in part 1, which is why we have a larger growth in part 1.

We found the "correct" number of iterations to be ~2000.

thread:list	per operation
		1 thread	8 threads	8 threads
		unproctected	pthread_mutex	spinlock
8:1		NULL	ns	80	ns	64	ns
4:1		NULL	ns	62	ns	55	ns
2:1		NULL	ns	37	ns	34	ns
1:1		6	ns	27	ns	24	ns
1:2		6	ns	24	ns	20	ns
1:4		6	ns	23	ns	19	ns
1:8		7	ns	26	ns	23	ns

2.3

Explain the the change in performance of the synchronized
methods as a function of the number of threads per list.

As the ratio of threads to lists increases, the time per operation
decreases because there are more locks available for more resources.
With higher numbers of lists, threads can hold a lock on one list
without preventing other threads from doing work on other lists.
If the number of threads was less than the number of lists, it was
similar to 1:1 as race conditions becomes less likely.

Explain why threads per list is a more interesting number
than threads (for this particular measurement).

The performance of our program depends on the amount of work available
for each thread, so the number of threads should be viewed relative to the
number of lists. For example, having 500 threads and 100 lists will not run
significantly faster than 5 threads and 1 list because there are not enough
lists to give to every thread, so every 4 out of 5 threads will sit idly
waiting for another thread to give up its lock on the list.

Part 3

3.1

Why must the mutex be held when pthread_cond_wait is called?

If it is not held, another thread could be holding it, so
when the current thread releases it with pthread_cond_wait,
a third thread will take control and two threads could be
accessing critical sections at once. Undefined behavior will result,
which we do not wish to happen.

Why must the mutex be released when the waiting thread is blocked?

The mutex must be released so that another thread can take control
and complete the task that the waiting thread is waiting for.

Why must the mutex be reacquired when the calling thread resumes?

The calling thread may access critical sections that we don't want
it to access unless it holds the mutex.

Why must this be done inside of pthread_cond_wait? Why can’t the
caller simply release the mutex before calling pthread_cond_wait?

Placing it within a system call will ensure that there is no race condition
between threads. As soon as the caller releases the mutex, another thread
can immediately take control before it gets around to calling pthread_cond_wait,
set the condition variable, and something else could then unset it before 
the calling thread is woken up. Even though the condition is set, the thread will
not be notified of the condition and will sit there waiting.
Holding the mutex into pthread_cond_wait will ensure that the caller has
finished its work and it is ready to receive the condition.

Can this be done in a user-mode implementation of pthread_cond_wait?
If so, how? If it can only be implemented by a system call, explain why?

No. A user-mode implementation does not have control over context switches,
so the caller can be suspended while the condition is met, possibly miss
it and hang forever. We must use a system call so that the thread will not
be suspended arbitrarily and ensure it will be woken up when the condition
is met.


